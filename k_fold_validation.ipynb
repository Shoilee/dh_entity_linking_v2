{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79ca17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarah_shoilee/opt/anaconda3/envs/py39deezy/lib/python3.9/site-packages/thefuzz/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from deezy_match_data_construction import construct_deezymatch_data, generate_test_data\n",
    "from DeezyMatch import train as dm_train\n",
    "from DeezyMatch import inference as dm_inference\n",
    "from DeezyMatch import combine_vecs\n",
    "from DeezyMatch import plot_log\n",
    "from DeezyMatch import candidate_ranker\n",
    "import os\n",
    "import pickle\n",
    "from fuzzy_string_matching import run as fuzzy_string_matching\n",
    "from result import result as compute_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a0242",
   "metadata": {},
   "source": [
    "TODO: How did we created gold-standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832efecb",
   "metadata": {},
   "source": [
    "TODO: write pseudo code for the steps in k_fold_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcd5d8",
   "metadata": {},
   "source": [
    "Number of entity in gold standard:\n",
    "\n",
    "for i in range(5): # considering k=5\n",
    "    - take 4 fold data for training and 1 for for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da84ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(file=\"pm_data/ccrdfconst/wikidata_human_name.pkl\", k=5):\n",
    "    master_df = pandas.read_pickle(file) # load 6178 entities\n",
    "\n",
    "    DataFrameDict = {i: pandas.DataFrame() for i in range(0,k)} # key = [0, 9], value dataframe chunks\n",
    "    resultDict = {i: {} for i in range(0,k)}\n",
    "    \n",
    "    try:\n",
    "        offset = round(len(master_df)/k)\n",
    "        for i in range(k):\n",
    "            DataFrameDict[i] = master_df[offset*i:offset*(i+1)]\n",
    "        \n",
    "        # TODO: run it for [0, 1, 2]\n",
    "        #for i in range(k):\n",
    "        for i in [0, 1, 2]:\n",
    "            train_sample = pandas.DataFrame()\n",
    "            test_sample = DataFrameDict[i]\n",
    "            for dict_index in range(0, k):\n",
    "                if dict_index == i:\n",
    "                    continue\n",
    "                train_sample = pandas.concat([train_sample, DataFrameDict[dict_index]], ignore_index=True)\n",
    "\n",
    "            # GENERATE TRAINING DATA\n",
    "            train_sample.to_pickle('k_fold_validation/training_sample.pkl')\n",
    "            construct_deezymatch_data('k_fold_validation/training_sample.pkl',  directory=\"k_fold_validation/\")\n",
    "\n",
    "            # GENERATE TEST DATA\n",
    "            test_sample.to_pickle('k_fold_validation/test_sample.pkl')\n",
    "            generate_test_data('k_fold_validation/test_sample.pkl', directory='k_fold_validation/')\n",
    "\n",
    "            # train a new model\n",
    "            dm_train(input_file_path=os.path.join(\"inputs\", \"input_dfm.yaml\"),\n",
    "                     dataset_path=os.path.join(\"k_fold_validation\", \"name_pairs.txt\"),\n",
    "                     model_name=\"test00\"+str(i))\n",
    "\n",
    "\n",
    "            # generate vectors for queries (specified in dataset_path) \n",
    "            # using a model stored at pretrained_model_path and pretrained_vocab_path \n",
    "            dm_inference(os.path.join(\"inputs\", \"input_dfm.yaml\"),\n",
    "                         dataset_path=os.path.join(\"k_fold_validation\", \"queries.txt\"), \n",
    "                         pretrained_model_path=os.path.join(\"models\", \"test00\"+str(i), \"test00\"+str(i)+\".model\"), \n",
    "                         pretrained_vocab_path=os.path.join(\"models\", \"test00\"+str(i), \"test00\"+str(i)+\".vocab\"),\n",
    "                         inference_mode=\"vect\",\n",
    "                         scenario=\"queries/test\")\n",
    "\n",
    "            # generate vectors for candidates (specified in dataset_path) \n",
    "            # using a model stored at pretrained_model_path and pretrained_vocab_path \n",
    "            dm_inference(os.path.join(\"inputs\", \"input_dfm.yaml\"),\n",
    "                         dataset_path=os.path.join(\"k_fold_validation\", \"candidates.txt\"), \n",
    "                         pretrained_model_path=os.path.join(\"models\", \"test00\"+str(i), \"test00\"+str(i)+\".model\"), \n",
    "                         pretrained_vocab_path=os.path.join(\"models\", \"test00\"+str(i), \"test00\"+str(i)+\".vocab\"),\n",
    "                         inference_mode=\"vect\",\n",
    "                         scenario=\"candidates/test\")\n",
    "\n",
    "            # combine vectors stored in queries/test and save them in combined/queries_test\n",
    "            combine_vecs(rnn_passes=['fwd', 'bwd'], \n",
    "                         input_scenario=os.path.join('queries', 'test'), \n",
    "                         output_scenario=os.path.join('combined', 'queries_test'), \n",
    "                         print_every=10)\n",
    "\n",
    "            combine_vecs(rnn_passes=['fwd', 'bwd'], \n",
    "                 input_scenario=os.path.join('candidates', 'test'), \n",
    "                 output_scenario=os.path.join('combined', 'candidates_test'), \n",
    "                 print_every=10)\n",
    "\n",
    "            # Select candidates based on L2-norm (aka faiss distance):\n",
    "            # find candidates from candidate_scenario \n",
    "            # for queries specified in query_scenario\n",
    "            candidates_pd = \\\n",
    "                candidate_ranker(query_scenario=os.path.join(\"combined\", \"queries_test\"),\n",
    "                                 candidate_scenario=os.path.join(\"combined\", \"candidates_test\"), \n",
    "                                 ranking_metric=\"faiss\", # two accepted value = ['cosine', faise]\n",
    "                                 selection_threshold=.5, \n",
    "                                 num_candidates=3, \n",
    "                                 search_size=10, \n",
    "                                 verbose=False,\n",
    "                                 use_predict=False,\n",
    "                                 output_path=os.path.join(\"ranker_results\", \"test_candidates_deezymatch\"), \n",
    "                                 pretrained_model_path=os.path.join(\"models\", \"test00\"+str(i), \"test00\"+str(i)+\".model\"), \n",
    "                                 pretrained_vocab_path=os.path.join(\"models\", \"test00\"+str(i), \"test00\"+str(i)+\".vocab\"))\n",
    "\n",
    "            fuzzy_string_matching(os.path.join(\"ranker_results\", \"test_candidates_deezymatch.pkl\"), os.path.join(\"k_fold_validation\", \"result.pkl\"), directory='k_fold_validation/')\n",
    "\n",
    "            total, retrieved, correct, r, p, f = compute_result(os.path.join(\"k_fold_validation\", \"result.pkl\"))\n",
    "\n",
    "\n",
    "            result = {\n",
    "                'instance': total,\n",
    "                'retrieved': retrieved, \n",
    "                'correct': correct, \n",
    "                'recall': r, \n",
    "                'precision': p, \n",
    "                'f-score': f\n",
    "            }\n",
    "\n",
    "            resultDict[i] = result\n",
    "    \n",
    "    finally:\n",
    "        with open('k_fold_validation/resultDict_faiss_3.pickle', 'wb') as handle:\n",
    "            pickle.dump(resultDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6884c18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/name_pairs.txt\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 24390 and False: 24390\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mSplitting the Dataset\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mfinish splitting the Dataset. User time: 0.026088953018188477\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msplits are as follow:\n",
      "train    34146\n",
      "test      7318\n",
      "val       7316\n",
      "Name: split, dtype: int64\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mstart creating a lookup table and convert characters to indices\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:12\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create vocabulary\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- convert tokens to indices\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create a lookup table for tokens\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- read list of characters from ./inputs/characters_v001.vocab\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- Length of vocabulary: 48409\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m**** (Bi-directional) GRU ****\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread inputs\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mcreate a two_parallel_rnns model\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mstart fitting parameters\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of batches: 3415\u001b[0m\n",
      "\u001b[92m2023-06-14 22:46:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of epochs: 5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1bae49f9554c09a254f3d5c42ec1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "====================\n",
      "Total number of params: 3079263\n",
      "\n",
      "two_parallel_rnns (\n",
      "  (emb): Embedding(48409, 60), weights=((48409, 60),), parameters=2904540\n",
      "  (rnn_1): GRU(60, 60, num_layers=2, dropout=0.01, bidirectional=True), weights=((180, 60), (180, 60), (180,), (180,), (180, 60), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,)), parameters=109440\n",
      "  (attn_step1): Linear(in_features=120, out_features=60, bias=True), weights=((60, 120), (60,)), parameters=7260\n",
      "  (attn_step2): Linear(in_features=60, out_features=1, bias=True), weights=((1, 60), (1,)), parameters=61\n",
      "  (fc1): Linear(in_features=480, out_features=120, bias=True), weights=((120, 480), (120,)), parameters=57720\n",
      "  (fc2): Linear(in_features=120, out_features=2, bias=True), weights=((2, 120), (2,)), parameters=242\n",
      ")\n",
      "====================\n",
      "\n",
      "\n",
      "\u001b[92m2023-06-14 22:55:47\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/14/2023_22:55:47 -- Epoch: 1/5; Train; loss: 0.220; acc: 0.904; precision: 0.889, recall: 0.924, macrof1: 0.904, weightedf1: 0.904\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 22:56:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/14/2023_22:56:02 -- Epoch: 1/5; Valid; loss: 0.111; acc: 0.959; precision: 0.966, recall: 0.952, macrof1: 0.959, weightedf1: 0.959\u001b[0m\n",
      "\u001b[92m2023-06-14 22:56:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:05:41\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/14/2023_23:05:41 -- Epoch: 2/5; Train; loss: 0.076; acc: 0.972; precision: 0.969, recall: 0.974, macrof1: 0.972, weightedf1: 0.972\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:05:55\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/14/2023_23:05:55 -- Epoch: 2/5; Valid; loss: 0.080; acc: 0.971; precision: 0.963, recall: 0.980, macrof1: 0.971, weightedf1: 0.971\u001b[0m\n",
      "\u001b[92m2023-06-14 23:05:55\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:15:49\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/14/2023_23:15:49 -- Epoch: 3/5; Train; loss: 0.038; acc: 0.986; precision: 0.985, recall: 0.988, macrof1: 0.986, weightedf1: 0.986\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:16:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/14/2023_23:16:03 -- Epoch: 3/5; Valid; loss: 0.073; acc: 0.975; precision: 0.979, recall: 0.971, macrof1: 0.975, weightedf1: 0.975\u001b[0m\n",
      "\u001b[92m2023-06-14 23:16:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:25:52\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/14/2023_23:25:52 -- Epoch: 4/5; Train; loss: 0.023; acc: 0.992; precision: 0.992, recall: 0.993, macrof1: 0.992, weightedf1: 0.992\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:26:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/14/2023_23:26:06 -- Epoch: 4/5; Valid; loss: 0.077; acc: 0.976; precision: 0.968, recall: 0.986, macrof1: 0.976, weightedf1: 0.976\u001b[0m\n",
      "\u001b[92m2023-06-14 23:26:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:36:00\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/14/2023_23:36:00 -- Epoch: 5/5; Train; loss: 0.020; acc: 0.993; precision: 0.993, recall: 0.993, macrof1: 0.993, weightedf1: 0.993\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:36:14\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/14/2023_23:36:14 -- Epoch: 5/5; Valid; loss: 0.127; acc: 0.966; precision: 0.940, recall: 0.996, macrof1: 0.966, weightedf1: 0.966\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:14\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:14\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model with least valid loss (checkpoint: 3) at ./models/test000/test000.model\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "User time: 2997.9100\n",
      "====================\n",
      "\u001b[92m2023-06-14 23:36:14\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:14\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:14\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/queries.txt\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 1236 and False: 0\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mskipping 0 lines\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:36:15\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32msave test-data-class: /Users/sarah_shoilee/PycharmProjects/entity_linking/queries/test/dataframe.df\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.8054769039154053 seconds ---\n",
      "\u001b[92m2023-06-14 23:36:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/candidates.txt\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:16\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 2770 and False: 0\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:17\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mskipping 22 lines\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-14 23:36:17\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32msave test-data-class: /Users/sarah_shoilee/PycharmProjects/entity_linking/candidates/test/dataframe.df\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.059619903564453 seconds ---\n",
      "\u001b[92m2023-06-14 23:36:20\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: queries/test/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:20\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:20\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from queries/test/embeddings/rnn_fwd*\n",
      "0000000 queries/test/embeddings/rnn_fwd_0\n",
      "0000010 queries/test/embeddings/rnn_fwd_10\n",
      "0000020 queries/test/embeddings/rnn_fwd_20\n",
      "0000030 queries/test/embeddings/rnn_fwd_30\n",
      "0000040 queries/test/embeddings/rnn_fwd_40\n",
      "0000050 queries/test/embeddings/rnn_fwd_50\n",
      "0000060 queries/test/embeddings/rnn_fwd_60\n",
      "0000070 queries/test/embeddings/rnn_fwd_70\n",
      "0000080 queries/test/embeddings/rnn_fwd_80\n",
      "0000090 queries/test/embeddings/rnn_fwd_90\n",
      "0000100 queries/test/embeddings/rnn_fwd_100\n",
      "0000110 queries/test/embeddings/rnn_fwd_110\n",
      "0000120 queries/test/embeddings/rnn_fwd_120\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 queries/test/embeddings/rnn_indxs_0\n",
      "0000010 queries/test/embeddings/rnn_indxs_10\n",
      "0000020 queries/test/embeddings/rnn_indxs_20\n",
      "0000030 queries/test/embeddings/rnn_indxs_30\n",
      "0000040 queries/test/embeddings/rnn_indxs_40\n",
      "0000050 queries/test/embeddings/rnn_indxs_50\n",
      "0000060 queries/test/embeddings/rnn_indxs_60\n",
      "0000070 queries/test/embeddings/rnn_indxs_70\n",
      "0000080 queries/test/embeddings/rnn_indxs_80\n",
      "0000090 queries/test/embeddings/rnn_indxs_90\n",
      "0000100 queries/test/embeddings/rnn_indxs_100\n",
      "0000110 queries/test/embeddings/rnn_indxs_110\n",
      "0000120 queries/test/embeddings/rnn_indxs_120\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from queries/test/embeddings/rnn_bwd*\n",
      "0000000 queries/test/embeddings/rnn_bwd_0\n",
      "0000010 queries/test/embeddings/rnn_bwd_10\n",
      "0000020 queries/test/embeddings/rnn_bwd_20\n",
      "0000030 queries/test/embeddings/rnn_bwd_30\n",
      "0000040 queries/test/embeddings/rnn_bwd_40\n",
      "0000050 queries/test/embeddings/rnn_bwd_50\n",
      "0000060 queries/test/embeddings/rnn_bwd_60\n",
      "0000070 queries/test/embeddings/rnn_bwd_70\n",
      "0000080 queries/test/embeddings/rnn_bwd_80\n",
      "0000090 queries/test/embeddings/rnn_bwd_90\n",
      "0000100 queries/test/embeddings/rnn_bwd_100\n",
      "0000110 queries/test/embeddings/rnn_bwd_110\n",
      "0000120 queries/test/embeddings/rnn_bwd_120\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 queries/test/embeddings/rnn_indxs_0\n",
      "0000010 queries/test/embeddings/rnn_indxs_10\n",
      "0000020 queries/test/embeddings/rnn_indxs_20\n",
      "0000030 queries/test/embeddings/rnn_indxs_30\n",
      "0000040 queries/test/embeddings/rnn_indxs_40\n",
      "0000050 queries/test/embeddings/rnn_indxs_50\n",
      "0000060 queries/test/embeddings/rnn_indxs_60\n",
      "0000070 queries/test/embeddings/rnn_indxs_70\n",
      "0000080 queries/test/embeddings/rnn_indxs_80\n",
      "0000090 queries/test/embeddings/rnn_indxs_90\n",
      "0000100 queries/test/embeddings/rnn_indxs_100\n",
      "0000110 queries/test/embeddings/rnn_indxs_110\n",
      "0000120 queries/test/embeddings/rnn_indxs_120\n",
      "--- 3067.1760392189026 seconds ---\n",
      "\u001b[92m2023-06-14 23:36:21\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: candidates/test/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:21\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-14 23:36:21\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from candidates/test/embeddings/rnn_fwd*\n",
      "0000000 candidates/test/embeddings/rnn_fwd_0\n",
      "0000010 candidates/test/embeddings/rnn_fwd_10\n",
      "0000020 candidates/test/embeddings/rnn_fwd_20\n",
      "0000030 candidates/test/embeddings/rnn_fwd_30\n",
      "0000040 candidates/test/embeddings/rnn_fwd_40\n",
      "0000050 candidates/test/embeddings/rnn_fwd_50\n",
      "0000060 candidates/test/embeddings/rnn_fwd_60\n",
      "0000070 candidates/test/embeddings/rnn_fwd_70\n",
      "0000080 candidates/test/embeddings/rnn_fwd_80\n",
      "0000090 candidates/test/embeddings/rnn_fwd_90\n",
      "0000100 candidates/test/embeddings/rnn_fwd_100\n",
      "0000110 candidates/test/embeddings/rnn_fwd_110\n",
      "0000120 candidates/test/embeddings/rnn_fwd_120\n",
      "0000130 candidates/test/embeddings/rnn_fwd_130\n",
      "0000140 candidates/test/embeddings/rnn_fwd_140\n",
      "0000150 candidates/test/embeddings/rnn_fwd_150\n",
      "0000160 candidates/test/embeddings/rnn_fwd_160\n",
      "0000170 candidates/test/embeddings/rnn_fwd_170\n",
      "0000180 candidates/test/embeddings/rnn_fwd_180\n",
      "0000190 candidates/test/embeddings/rnn_fwd_190\n",
      "0000200 candidates/test/embeddings/rnn_fwd_200\n",
      "0000210 candidates/test/embeddings/rnn_fwd_210\n",
      "0000220 candidates/test/embeddings/rnn_fwd_220\n",
      "0000230 candidates/test/embeddings/rnn_fwd_230\n",
      "0000240 candidates/test/embeddings/rnn_fwd_240\n",
      "0000250 candidates/test/embeddings/rnn_fwd_250\n",
      "0000260 candidates/test/embeddings/rnn_fwd_260\n",
      "0000270 candidates/test/embeddings/rnn_fwd_270\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 candidates/test/embeddings/rnn_indxs_0\n",
      "0000010 candidates/test/embeddings/rnn_indxs_10\n",
      "0000020 candidates/test/embeddings/rnn_indxs_20\n",
      "0000030 candidates/test/embeddings/rnn_indxs_30\n",
      "0000040 candidates/test/embeddings/rnn_indxs_40\n",
      "0000050 candidates/test/embeddings/rnn_indxs_50\n",
      "0000060 candidates/test/embeddings/rnn_indxs_60\n",
      "0000070 candidates/test/embeddings/rnn_indxs_70\n",
      "0000080 candidates/test/embeddings/rnn_indxs_80\n",
      "0000090 candidates/test/embeddings/rnn_indxs_90\n",
      "0000100 candidates/test/embeddings/rnn_indxs_100\n",
      "0000110 candidates/test/embeddings/rnn_indxs_110\n",
      "0000120 candidates/test/embeddings/rnn_indxs_120\n",
      "0000130 candidates/test/embeddings/rnn_indxs_130\n",
      "0000140 candidates/test/embeddings/rnn_indxs_140\n",
      "0000150 candidates/test/embeddings/rnn_indxs_150\n",
      "0000160 candidates/test/embeddings/rnn_indxs_160\n",
      "0000170 candidates/test/embeddings/rnn_indxs_170\n",
      "0000180 candidates/test/embeddings/rnn_indxs_180\n",
      "0000190 candidates/test/embeddings/rnn_indxs_190\n",
      "0000200 candidates/test/embeddings/rnn_indxs_200\n",
      "0000210 candidates/test/embeddings/rnn_indxs_210\n",
      "0000220 candidates/test/embeddings/rnn_indxs_220\n",
      "0000230 candidates/test/embeddings/rnn_indxs_230\n",
      "0000240 candidates/test/embeddings/rnn_indxs_240\n",
      "0000250 candidates/test/embeddings/rnn_indxs_250\n",
      "0000260 candidates/test/embeddings/rnn_indxs_260\n",
      "0000270 candidates/test/embeddings/rnn_indxs_270\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from candidates/test/embeddings/rnn_bwd*\n",
      "0000000 candidates/test/embeddings/rnn_bwd_0\n",
      "0000010 candidates/test/embeddings/rnn_bwd_10\n",
      "0000020 candidates/test/embeddings/rnn_bwd_20\n",
      "0000030 candidates/test/embeddings/rnn_bwd_30\n",
      "0000040 candidates/test/embeddings/rnn_bwd_40\n",
      "0000050 candidates/test/embeddings/rnn_bwd_50\n",
      "0000060 candidates/test/embeddings/rnn_bwd_60\n",
      "0000070 candidates/test/embeddings/rnn_bwd_70\n",
      "0000080 candidates/test/embeddings/rnn_bwd_80\n",
      "0000090 candidates/test/embeddings/rnn_bwd_90\n",
      "0000100 candidates/test/embeddings/rnn_bwd_100\n",
      "0000110 candidates/test/embeddings/rnn_bwd_110\n",
      "0000120 candidates/test/embeddings/rnn_bwd_120\n",
      "0000130 candidates/test/embeddings/rnn_bwd_130\n",
      "0000140 candidates/test/embeddings/rnn_bwd_140\n",
      "0000150 candidates/test/embeddings/rnn_bwd_150\n",
      "0000160 candidates/test/embeddings/rnn_bwd_160\n",
      "0000170 candidates/test/embeddings/rnn_bwd_170\n",
      "0000180 candidates/test/embeddings/rnn_bwd_180\n",
      "0000190 candidates/test/embeddings/rnn_bwd_190\n",
      "0000200 candidates/test/embeddings/rnn_bwd_200\n",
      "0000210 candidates/test/embeddings/rnn_bwd_210\n",
      "0000220 candidates/test/embeddings/rnn_bwd_220\n",
      "0000230 candidates/test/embeddings/rnn_bwd_230\n",
      "0000240 candidates/test/embeddings/rnn_bwd_240\n",
      "0000250 candidates/test/embeddings/rnn_bwd_250\n",
      "0000260 candidates/test/embeddings/rnn_bwd_260\n",
      "0000270 candidates/test/embeddings/rnn_bwd_270\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 candidates/test/embeddings/rnn_indxs_0\n",
      "0000010 candidates/test/embeddings/rnn_indxs_10\n",
      "0000020 candidates/test/embeddings/rnn_indxs_20\n",
      "0000030 candidates/test/embeddings/rnn_indxs_30\n",
      "0000040 candidates/test/embeddings/rnn_indxs_40\n",
      "0000050 candidates/test/embeddings/rnn_indxs_50\n",
      "0000060 candidates/test/embeddings/rnn_indxs_60\n",
      "0000070 candidates/test/embeddings/rnn_indxs_70\n",
      "0000080 candidates/test/embeddings/rnn_indxs_80\n",
      "0000090 candidates/test/embeddings/rnn_indxs_90\n",
      "0000100 candidates/test/embeddings/rnn_indxs_100\n",
      "0000110 candidates/test/embeddings/rnn_indxs_110\n",
      "0000120 candidates/test/embeddings/rnn_indxs_120\n",
      "0000130 candidates/test/embeddings/rnn_indxs_130\n",
      "0000140 candidates/test/embeddings/rnn_indxs_140\n",
      "0000150 candidates/test/embeddings/rnn_indxs_150\n",
      "0000160 candidates/test/embeddings/rnn_indxs_160\n",
      "0000170 candidates/test/embeddings/rnn_indxs_170\n",
      "0000180 candidates/test/embeddings/rnn_indxs_180\n",
      "0000190 candidates/test/embeddings/rnn_indxs_190\n",
      "0000200 candidates/test/embeddings/rnn_indxs_200\n",
      "0000210 candidates/test/embeddings/rnn_indxs_210\n",
      "0000220 candidates/test/embeddings/rnn_indxs_220\n",
      "0000230 candidates/test/embeddings/rnn_indxs_230\n",
      "0000240 candidates/test/embeddings/rnn_indxs_240\n",
      "0000250 candidates/test/embeddings/rnn_indxs_250\n",
      "0000260 candidates/test/embeddings/rnn_indxs_260\n",
      "0000270 candidates/test/embeddings/rnn_indxs_270\n",
      "--- 3067.5802342891693 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1236it [16:35,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total query: 1236 \n",
      "Correct correspondence count: 580\n",
      "Recall: 0.4692556634304207\n",
      "\n",
      "\n",
      "Total query: 1236\n",
      "Total retrieved: 698\n",
      "Correct correspondence count: 580 \n",
      "Precision: 0.830945558739255\n",
      "\n",
      "\n",
      "F-measure: 0.5997931747673216\n",
      "\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/name_pairs.txt\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 23951 and False: 23951\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mSplitting the Dataset\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mfinish splitting the Dataset. User time: 0.021077871322631836\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msplits are as follow:\n",
      "train    33532\n",
      "test      7186\n",
      "val       7184\n",
      "Name: split, dtype: int64\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mstart creating a lookup table and convert characters to indices\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create vocabulary\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:09\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- convert tokens to indices\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:09\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create a lookup table for tokens\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:09\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- read list of characters from ./inputs/characters_v001.vocab\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:09\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- Length of vocabulary: 46260\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m**** (Bi-directional) GRU ****\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread inputs\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mcreate a two_parallel_rnns model\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mstart fitting parameters\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of batches: 3354\u001b[0m\n",
      "\u001b[92m2023-06-14 23:53:11\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of epochs: 5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bbdc95e5e242c49623666dc57dc464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "====================\n",
      "Total number of params: 2950323\n",
      "\n",
      "two_parallel_rnns (\n",
      "  (emb): Embedding(46260, 60), weights=((46260, 60),), parameters=2775600\n",
      "  (rnn_1): GRU(60, 60, num_layers=2, dropout=0.01, bidirectional=True), weights=((180, 60), (180, 60), (180,), (180,), (180, 60), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,)), parameters=109440\n",
      "  (attn_step1): Linear(in_features=120, out_features=60, bias=True), weights=((60, 120), (60,)), parameters=7260\n",
      "  (attn_step2): Linear(in_features=60, out_features=1, bias=True), weights=((1, 60), (1,)), parameters=61\n",
      "  (fc1): Linear(in_features=480, out_features=120, bias=True), weights=((120, 480), (120,)), parameters=57720\n",
      "  (fc2): Linear(in_features=120, out_features=2, bias=True), weights=((2, 120), (2,)), parameters=242\n",
      ")\n",
      "====================\n",
      "\n",
      "\n",
      "\u001b[92m2023-06-15 00:02:45\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_00:02:45 -- Epoch: 1/5; Train; loss: 0.222; acc: 0.904; precision: 0.892, recall: 0.920, macrof1: 0.904, weightedf1: 0.904\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:02:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_00:02:59 -- Epoch: 1/5; Valid; loss: 0.137; acc: 0.951; precision: 0.949, recall: 0.954, macrof1: 0.951, weightedf1: 0.951\u001b[0m\n",
      "\u001b[92m2023-06-15 00:02:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:12:41\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_00:12:41 -- Epoch: 2/5; Train; loss: 0.079; acc: 0.972; precision: 0.969, recall: 0.975, macrof1: 0.972, weightedf1: 0.972\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:12:55\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_00:12:55 -- Epoch: 2/5; Valid; loss: 0.087; acc: 0.969; precision: 0.962, recall: 0.977, macrof1: 0.969, weightedf1: 0.969\u001b[0m\n",
      "\u001b[92m2023-06-15 00:12:55\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:22:41\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_00:22:41 -- Epoch: 3/5; Train; loss: 0.038; acc: 0.987; precision: 0.986, recall: 0.989, macrof1: 0.987, weightedf1: 0.987\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:22:55\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_00:22:55 -- Epoch: 3/5; Valid; loss: 0.093; acc: 0.971; precision: 0.963, recall: 0.979, macrof1: 0.971, weightedf1: 0.971\u001b[0m\n",
      "\u001b[92m2023-06-15 00:22:55\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:32:42\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_00:32:42 -- Epoch: 4/5; Train; loss: 0.023; acc: 0.992; precision: 0.992, recall: 0.993, macrof1: 0.992, weightedf1: 0.992\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:32:56\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_00:32:56 -- Epoch: 4/5; Valid; loss: 0.109; acc: 0.970; precision: 0.973, recall: 0.967, macrof1: 0.970, weightedf1: 0.970\u001b[0m\n",
      "\u001b[92m2023-06-15 00:32:56\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:42:47\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_00:42:47 -- Epoch: 5/5; Train; loss: 0.020; acc: 0.993; precision: 0.993, recall: 0.993, macrof1: 0.993, weightedf1: 0.993\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:43:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_00:43:01 -- Epoch: 5/5; Valid; loss: 0.101; acc: 0.974; precision: 0.973, recall: 0.976, macrof1: 0.974, weightedf1: 0.974\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model with least valid loss (checkpoint: 2) at ./models/test001/test001.model\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "User time: 2990.8811\n",
      "====================\n",
      "\u001b[92m2023-06-15 00:43:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/queries.txt\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 1236 and False: 0\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mskipping 0 lines\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:43:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32msave test-data-class: /Users/sarah_shoilee/PycharmProjects/entity_linking/queries/test/dataframe.df\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.788862943649292 seconds ---\n",
      "\u001b[92m2023-06-15 00:43:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/candidates.txt\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 3242 and False: 0\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:04\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mskipping 94 lines\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 00:43:04\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32msave test-data-class: /Users/sarah_shoilee/PycharmProjects/entity_linking/candidates/test/dataframe.df\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.9342870712280273 seconds ---\n",
      "\u001b[92m2023-06-15 00:43:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: queries/test/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from queries/test/embeddings/rnn_fwd*\n",
      "0000000 queries/test/embeddings/rnn_fwd_0\n",
      "0000010 queries/test/embeddings/rnn_fwd_10\n",
      "0000020 queries/test/embeddings/rnn_fwd_20\n",
      "0000030 queries/test/embeddings/rnn_fwd_30\n",
      "0000040 queries/test/embeddings/rnn_fwd_40\n",
      "0000050 queries/test/embeddings/rnn_fwd_50\n",
      "0000060 queries/test/embeddings/rnn_fwd_60\n",
      "0000070 queries/test/embeddings/rnn_fwd_70\n",
      "0000080 queries/test/embeddings/rnn_fwd_80\n",
      "0000090 queries/test/embeddings/rnn_fwd_90\n",
      "0000100 queries/test/embeddings/rnn_fwd_100\n",
      "0000110 queries/test/embeddings/rnn_fwd_110\n",
      "0000120 queries/test/embeddings/rnn_fwd_120\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 queries/test/embeddings/rnn_indxs_0\n",
      "0000010 queries/test/embeddings/rnn_indxs_10\n",
      "0000020 queries/test/embeddings/rnn_indxs_20\n",
      "0000030 queries/test/embeddings/rnn_indxs_30\n",
      "0000040 queries/test/embeddings/rnn_indxs_40\n",
      "0000050 queries/test/embeddings/rnn_indxs_50\n",
      "0000060 queries/test/embeddings/rnn_indxs_60\n",
      "0000070 queries/test/embeddings/rnn_indxs_70\n",
      "0000080 queries/test/embeddings/rnn_indxs_80\n",
      "0000090 queries/test/embeddings/rnn_indxs_90\n",
      "0000100 queries/test/embeddings/rnn_indxs_100\n",
      "0000110 queries/test/embeddings/rnn_indxs_110\n",
      "0000120 queries/test/embeddings/rnn_indxs_120\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from queries/test/embeddings/rnn_bwd*\n",
      "0000000 queries/test/embeddings/rnn_bwd_0\n",
      "0000010 queries/test/embeddings/rnn_bwd_10\n",
      "0000020 queries/test/embeddings/rnn_bwd_20\n",
      "0000030 queries/test/embeddings/rnn_bwd_30\n",
      "0000040 queries/test/embeddings/rnn_bwd_40\n",
      "0000050 queries/test/embeddings/rnn_bwd_50\n",
      "0000060 queries/test/embeddings/rnn_bwd_60\n",
      "0000070 queries/test/embeddings/rnn_bwd_70\n",
      "0000080 queries/test/embeddings/rnn_bwd_80\n",
      "0000090 queries/test/embeddings/rnn_bwd_90\n",
      "0000100 queries/test/embeddings/rnn_bwd_100\n",
      "0000110 queries/test/embeddings/rnn_bwd_110\n",
      "0000120 queries/test/embeddings/rnn_bwd_120\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 queries/test/embeddings/rnn_indxs_0\n",
      "0000010 queries/test/embeddings/rnn_indxs_10\n",
      "0000020 queries/test/embeddings/rnn_indxs_20\n",
      "0000030 queries/test/embeddings/rnn_indxs_30\n",
      "0000040 queries/test/embeddings/rnn_indxs_40\n",
      "0000050 queries/test/embeddings/rnn_indxs_50\n",
      "0000060 queries/test/embeddings/rnn_indxs_60\n",
      "0000070 queries/test/embeddings/rnn_indxs_70\n",
      "0000080 queries/test/embeddings/rnn_indxs_80\n",
      "0000090 queries/test/embeddings/rnn_indxs_90\n",
      "0000100 queries/test/embeddings/rnn_indxs_100\n",
      "0000110 queries/test/embeddings/rnn_indxs_110\n",
      "0000120 queries/test/embeddings/rnn_indxs_120\n",
      "--- 7074.041241168976 seconds ---\n",
      "\u001b[92m2023-06-15 00:43:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: candidates/test/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 00:43:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from candidates/test/embeddings/rnn_fwd*\n",
      "0000000 candidates/test/embeddings/rnn_fwd_0\n",
      "0000010 candidates/test/embeddings/rnn_fwd_10\n",
      "0000020 candidates/test/embeddings/rnn_fwd_20\n",
      "0000030 candidates/test/embeddings/rnn_fwd_30\n",
      "0000040 candidates/test/embeddings/rnn_fwd_40\n",
      "0000050 candidates/test/embeddings/rnn_fwd_50\n",
      "0000060 candidates/test/embeddings/rnn_fwd_60\n",
      "0000070 candidates/test/embeddings/rnn_fwd_70\n",
      "0000080 candidates/test/embeddings/rnn_fwd_80\n",
      "0000090 candidates/test/embeddings/rnn_fwd_90\n",
      "0000100 candidates/test/embeddings/rnn_fwd_100\n",
      "0000110 candidates/test/embeddings/rnn_fwd_110\n",
      "0000120 candidates/test/embeddings/rnn_fwd_120\n",
      "0000130 candidates/test/embeddings/rnn_fwd_130\n",
      "0000140 candidates/test/embeddings/rnn_fwd_140\n",
      "0000150 candidates/test/embeddings/rnn_fwd_150\n",
      "0000160 candidates/test/embeddings/rnn_fwd_160\n",
      "0000170 candidates/test/embeddings/rnn_fwd_170\n",
      "0000180 candidates/test/embeddings/rnn_fwd_180\n",
      "0000190 candidates/test/embeddings/rnn_fwd_190\n",
      "0000200 candidates/test/embeddings/rnn_fwd_200\n",
      "0000210 candidates/test/embeddings/rnn_fwd_210\n",
      "0000220 candidates/test/embeddings/rnn_fwd_220\n",
      "0000230 candidates/test/embeddings/rnn_fwd_230\n",
      "0000240 candidates/test/embeddings/rnn_fwd_240\n",
      "0000250 candidates/test/embeddings/rnn_fwd_250\n",
      "0000260 candidates/test/embeddings/rnn_fwd_260\n",
      "0000270 candidates/test/embeddings/rnn_fwd_270\n",
      "0000280 candidates/test/embeddings/rnn_fwd_280\n",
      "0000290 candidates/test/embeddings/rnn_fwd_290\n",
      "0000300 candidates/test/embeddings/rnn_fwd_300\n",
      "0000310 candidates/test/embeddings/rnn_fwd_310\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 candidates/test/embeddings/rnn_indxs_0\n",
      "0000010 candidates/test/embeddings/rnn_indxs_10\n",
      "0000020 candidates/test/embeddings/rnn_indxs_20\n",
      "0000030 candidates/test/embeddings/rnn_indxs_30\n",
      "0000040 candidates/test/embeddings/rnn_indxs_40\n",
      "0000050 candidates/test/embeddings/rnn_indxs_50\n",
      "0000060 candidates/test/embeddings/rnn_indxs_60\n",
      "0000070 candidates/test/embeddings/rnn_indxs_70\n",
      "0000080 candidates/test/embeddings/rnn_indxs_80\n",
      "0000090 candidates/test/embeddings/rnn_indxs_90\n",
      "0000100 candidates/test/embeddings/rnn_indxs_100\n",
      "0000110 candidates/test/embeddings/rnn_indxs_110\n",
      "0000120 candidates/test/embeddings/rnn_indxs_120\n",
      "0000130 candidates/test/embeddings/rnn_indxs_130\n",
      "0000140 candidates/test/embeddings/rnn_indxs_140\n",
      "0000150 candidates/test/embeddings/rnn_indxs_150\n",
      "0000160 candidates/test/embeddings/rnn_indxs_160\n",
      "0000170 candidates/test/embeddings/rnn_indxs_170\n",
      "0000180 candidates/test/embeddings/rnn_indxs_180\n",
      "0000190 candidates/test/embeddings/rnn_indxs_190\n",
      "0000200 candidates/test/embeddings/rnn_indxs_200\n",
      "0000210 candidates/test/embeddings/rnn_indxs_210\n",
      "0000220 candidates/test/embeddings/rnn_indxs_220\n",
      "0000230 candidates/test/embeddings/rnn_indxs_230\n",
      "0000240 candidates/test/embeddings/rnn_indxs_240\n",
      "0000250 candidates/test/embeddings/rnn_indxs_250\n",
      "0000260 candidates/test/embeddings/rnn_indxs_260\n",
      "0000270 candidates/test/embeddings/rnn_indxs_270\n",
      "0000280 candidates/test/embeddings/rnn_indxs_280\n",
      "0000290 candidates/test/embeddings/rnn_indxs_290\n",
      "0000300 candidates/test/embeddings/rnn_indxs_300\n",
      "0000310 candidates/test/embeddings/rnn_indxs_310\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from candidates/test/embeddings/rnn_bwd*\n",
      "0000000 candidates/test/embeddings/rnn_bwd_0\n",
      "0000010 candidates/test/embeddings/rnn_bwd_10\n",
      "0000020 candidates/test/embeddings/rnn_bwd_20\n",
      "0000030 candidates/test/embeddings/rnn_bwd_30\n",
      "0000040 candidates/test/embeddings/rnn_bwd_40\n",
      "0000050 candidates/test/embeddings/rnn_bwd_50\n",
      "0000060 candidates/test/embeddings/rnn_bwd_60\n",
      "0000070 candidates/test/embeddings/rnn_bwd_70\n",
      "0000080 candidates/test/embeddings/rnn_bwd_80\n",
      "0000090 candidates/test/embeddings/rnn_bwd_90\n",
      "0000100 candidates/test/embeddings/rnn_bwd_100\n",
      "0000110 candidates/test/embeddings/rnn_bwd_110\n",
      "0000120 candidates/test/embeddings/rnn_bwd_120\n",
      "0000130 candidates/test/embeddings/rnn_bwd_130\n",
      "0000140 candidates/test/embeddings/rnn_bwd_140\n",
      "0000150 candidates/test/embeddings/rnn_bwd_150\n",
      "0000160 candidates/test/embeddings/rnn_bwd_160\n",
      "0000170 candidates/test/embeddings/rnn_bwd_170\n",
      "0000180 candidates/test/embeddings/rnn_bwd_180\n",
      "0000190 candidates/test/embeddings/rnn_bwd_190\n",
      "0000200 candidates/test/embeddings/rnn_bwd_200\n",
      "0000210 candidates/test/embeddings/rnn_bwd_210\n",
      "0000220 candidates/test/embeddings/rnn_bwd_220\n",
      "0000230 candidates/test/embeddings/rnn_bwd_230\n",
      "0000240 candidates/test/embeddings/rnn_bwd_240\n",
      "0000250 candidates/test/embeddings/rnn_bwd_250\n",
      "0000260 candidates/test/embeddings/rnn_bwd_260\n",
      "0000270 candidates/test/embeddings/rnn_bwd_270\n",
      "0000280 candidates/test/embeddings/rnn_bwd_280\n",
      "0000290 candidates/test/embeddings/rnn_bwd_290\n",
      "0000300 candidates/test/embeddings/rnn_bwd_300\n",
      "0000310 candidates/test/embeddings/rnn_bwd_310\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 candidates/test/embeddings/rnn_indxs_0\n",
      "0000010 candidates/test/embeddings/rnn_indxs_10\n",
      "0000020 candidates/test/embeddings/rnn_indxs_20\n",
      "0000030 candidates/test/embeddings/rnn_indxs_30\n",
      "0000040 candidates/test/embeddings/rnn_indxs_40\n",
      "0000050 candidates/test/embeddings/rnn_indxs_50\n",
      "0000060 candidates/test/embeddings/rnn_indxs_60\n",
      "0000070 candidates/test/embeddings/rnn_indxs_70\n",
      "0000080 candidates/test/embeddings/rnn_indxs_80\n",
      "0000090 candidates/test/embeddings/rnn_indxs_90\n",
      "0000100 candidates/test/embeddings/rnn_indxs_100\n",
      "0000110 candidates/test/embeddings/rnn_indxs_110\n",
      "0000120 candidates/test/embeddings/rnn_indxs_120\n",
      "0000130 candidates/test/embeddings/rnn_indxs_130\n",
      "0000140 candidates/test/embeddings/rnn_indxs_140\n",
      "0000150 candidates/test/embeddings/rnn_indxs_150\n",
      "0000160 candidates/test/embeddings/rnn_indxs_160\n",
      "0000170 candidates/test/embeddings/rnn_indxs_170\n",
      "0000180 candidates/test/embeddings/rnn_indxs_180\n",
      "0000190 candidates/test/embeddings/rnn_indxs_190\n",
      "0000200 candidates/test/embeddings/rnn_indxs_200\n",
      "0000210 candidates/test/embeddings/rnn_indxs_210\n",
      "0000220 candidates/test/embeddings/rnn_indxs_220\n",
      "0000230 candidates/test/embeddings/rnn_indxs_230\n",
      "0000240 candidates/test/embeddings/rnn_indxs_240\n",
      "0000250 candidates/test/embeddings/rnn_indxs_250\n",
      "0000260 candidates/test/embeddings/rnn_indxs_260\n",
      "0000270 candidates/test/embeddings/rnn_indxs_270\n",
      "0000280 candidates/test/embeddings/rnn_indxs_280\n",
      "0000290 candidates/test/embeddings/rnn_indxs_290\n",
      "0000300 candidates/test/embeddings/rnn_indxs_300\n",
      "0000310 candidates/test/embeddings/rnn_indxs_310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7074.862668037415 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1236it [15:40,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total query: 1236 \n",
      "Correct correspondence count: 534\n",
      "Recall: 0.4320388349514563\n",
      "\n",
      "\n",
      "Total query: 1236\n",
      "Total retrieved: 663\n",
      "Correct correspondence count: 534 \n",
      "Precision: 0.8054298642533937\n",
      "\n",
      "\n",
      "F-measure: 0.5624012638230648\n",
      "\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/name_pairs.txt\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 26155 and False: 26155\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mSplitting the Dataset\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mfinish splitting the Dataset. User time: 0.02371525764465332\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msplits are as follow:\n",
      "train    36616\n",
      "test      7848\n",
      "val       7846\n",
      "Name: split, dtype: int64\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mstart creating a lookup table and convert characters to indices\u001b[0m\n",
      "\u001b[92m2023-06-15 00:58:59\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create vocabulary\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- convert tokens to indices\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- create a lookup table for tokens\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- read list of characters from ./inputs/characters_v001.vocab\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:02\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32m-- Length of vocabulary: 49459\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m**** (Bi-directional) GRU ****\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[95m******************************\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread inputs\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mcreate a two_parallel_rnns model\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mstart fitting parameters\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of batches: 3662\u001b[0m\n",
      "\u001b[92m2023-06-15 00:59:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mNumber of epochs: 5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e78fc0f4c34568b53b053a73a39480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "====================\n",
      "Total number of params: 3142263\n",
      "\n",
      "two_parallel_rnns (\n",
      "  (emb): Embedding(49459, 60), weights=((49459, 60),), parameters=2967540\n",
      "  (rnn_1): GRU(60, 60, num_layers=2, dropout=0.01, bidirectional=True), weights=((180, 60), (180, 60), (180,), (180,), (180, 60), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,), (180, 120), (180, 60), (180,), (180,)), parameters=109440\n",
      "  (attn_step1): Linear(in_features=120, out_features=60, bias=True), weights=((60, 120), (60,)), parameters=7260\n",
      "  (attn_step2): Linear(in_features=60, out_features=1, bias=True), weights=((1, 60), (1,)), parameters=61\n",
      "  (fc1): Linear(in_features=480, out_features=120, bias=True), weights=((120, 480), (120,)), parameters=57720\n",
      "  (fc2): Linear(in_features=120, out_features=2, bias=True), weights=((2, 120), (2,)), parameters=242\n",
      ")\n",
      "====================\n",
      "\n",
      "\n",
      "\u001b[92m2023-06-15 01:09:52\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_01:09:52 -- Epoch: 1/5; Train; loss: 0.222; acc: 0.903; precision: 0.895, recall: 0.915, macrof1: 0.903, weightedf1: 0.903\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:10:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_01:10:07 -- Epoch: 1/5; Valid; loss: 0.100; acc: 0.963; precision: 0.967, recall: 0.958, macrof1: 0.963, weightedf1: 0.963\u001b[0m\n",
      "\u001b[92m2023-06-15 01:10:07\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:21:08\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_01:21:08 -- Epoch: 2/5; Train; loss: 0.070; acc: 0.974; precision: 0.972, recall: 0.976, macrof1: 0.974, weightedf1: 0.974\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:21:23\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_01:21:23 -- Epoch: 2/5; Valid; loss: 0.067; acc: 0.974; precision: 0.967, recall: 0.981, macrof1: 0.974, weightedf1: 0.974\u001b[0m\n",
      "\u001b[92m2023-06-15 01:21:23\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:32:23\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_01:32:23 -- Epoch: 3/5; Train; loss: 0.034; acc: 0.988; precision: 0.987, recall: 0.989, macrof1: 0.988, weightedf1: 0.988\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:32:39\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_01:32:39 -- Epoch: 3/5; Valid; loss: 0.118; acc: 0.962; precision: 0.933, recall: 0.995, macrof1: 0.962, weightedf1: 0.962\u001b[0m\n",
      "\u001b[92m2023-06-15 01:32:39\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:43:37\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_01:43:37 -- Epoch: 4/5; Train; loss: 0.021; acc: 0.993; precision: 0.992, recall: 0.993, macrof1: 0.993, weightedf1: 0.993\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:43:53\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_01:43:53 -- Epoch: 4/5; Valid; loss: 0.093; acc: 0.975; precision: 0.969, recall: 0.981, macrof1: 0.975, weightedf1: 0.975\u001b[0m\n",
      "\u001b[92m2023-06-15 01:43:53\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:54:45\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[0;33m06/15/2023_01:54:45 -- Epoch: 5/5; Train; loss: 0.018; acc: 0.993; precision: 0.992, recall: 0.994, macrof1: 0.993, weightedf1: 0.993\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31m06/15/2023_01:55:01 -- Epoch: 5/5; Valid; loss: 0.081; acc: 0.978; precision: 0.974, recall: 0.982, macrof1: 0.978, weightedf1: 0.978\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32msaving the model with least valid loss (checkpoint: 2) at ./models/test002/test002.model\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "User time: 3357.9153\n",
      "====================\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/queries.txt\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 1236 and False: 0\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mskipping 0 lines\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:55:01\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32msave test-data-class: /Users/sarah_shoilee/PycharmProjects/entity_linking/queries/test/dataframe.df\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.5553290843963623 seconds ---\n",
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: inputs/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread CSV file: k_fold_validation/candidates.txt\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mnumber of labels, True: 2304 and False: 0\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mskipping 24 lines\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2023-06-15 01:55:03\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32msave test-data-class: /Users/sarah_shoilee/PycharmProjects/entity_linking/candidates/test/dataframe.df\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.7814338207244873 seconds ---\n",
      "\u001b[92m2023-06-15 01:55:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: queries/test/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from queries/test/embeddings/rnn_fwd*\n",
      "0000000 queries/test/embeddings/rnn_fwd_0\n",
      "0000010 queries/test/embeddings/rnn_fwd_10\n",
      "0000020 queries/test/embeddings/rnn_fwd_20\n",
      "0000030 queries/test/embeddings/rnn_fwd_30\n",
      "0000040 queries/test/embeddings/rnn_fwd_40\n",
      "0000050 queries/test/embeddings/rnn_fwd_50\n",
      "0000060 queries/test/embeddings/rnn_fwd_60\n",
      "0000070 queries/test/embeddings/rnn_fwd_70\n",
      "0000080 queries/test/embeddings/rnn_fwd_80\n",
      "0000090 queries/test/embeddings/rnn_fwd_90\n",
      "0000100 queries/test/embeddings/rnn_fwd_100\n",
      "0000110 queries/test/embeddings/rnn_fwd_110\n",
      "0000120 queries/test/embeddings/rnn_fwd_120\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 queries/test/embeddings/rnn_indxs_0\n",
      "0000010 queries/test/embeddings/rnn_indxs_10\n",
      "0000020 queries/test/embeddings/rnn_indxs_20\n",
      "0000030 queries/test/embeddings/rnn_indxs_30\n",
      "0000040 queries/test/embeddings/rnn_indxs_40\n",
      "0000050 queries/test/embeddings/rnn_indxs_50\n",
      "0000060 queries/test/embeddings/rnn_indxs_60\n",
      "0000070 queries/test/embeddings/rnn_indxs_70\n",
      "0000080 queries/test/embeddings/rnn_indxs_80\n",
      "0000090 queries/test/embeddings/rnn_indxs_90\n",
      "0000100 queries/test/embeddings/rnn_indxs_100\n",
      "0000110 queries/test/embeddings/rnn_indxs_110\n",
      "0000120 queries/test/embeddings/rnn_indxs_120\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from queries/test/embeddings/rnn_bwd*\n",
      "0000000 queries/test/embeddings/rnn_bwd_0\n",
      "0000010 queries/test/embeddings/rnn_bwd_10\n",
      "0000020 queries/test/embeddings/rnn_bwd_20\n",
      "0000030 queries/test/embeddings/rnn_bwd_30\n",
      "0000040 queries/test/embeddings/rnn_bwd_40\n",
      "0000050 queries/test/embeddings/rnn_bwd_50\n",
      "0000060 queries/test/embeddings/rnn_bwd_60\n",
      "0000070 queries/test/embeddings/rnn_bwd_70\n",
      "0000080 queries/test/embeddings/rnn_bwd_80\n",
      "0000090 queries/test/embeddings/rnn_bwd_90\n",
      "0000100 queries/test/embeddings/rnn_bwd_100\n",
      "0000110 queries/test/embeddings/rnn_bwd_110\n",
      "0000120 queries/test/embeddings/rnn_bwd_120\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 queries/test/embeddings/rnn_indxs_0\n",
      "0000010 queries/test/embeddings/rnn_indxs_10\n",
      "0000020 queries/test/embeddings/rnn_indxs_20\n",
      "0000030 queries/test/embeddings/rnn_indxs_30\n",
      "0000040 queries/test/embeddings/rnn_indxs_40\n",
      "0000050 queries/test/embeddings/rnn_indxs_50\n",
      "0000060 queries/test/embeddings/rnn_indxs_60\n",
      "0000070 queries/test/embeddings/rnn_indxs_70\n",
      "0000080 queries/test/embeddings/rnn_indxs_80\n",
      "0000090 queries/test/embeddings/rnn_indxs_90\n",
      "0000100 queries/test/embeddings/rnn_indxs_100\n",
      "0000110 queries/test/embeddings/rnn_indxs_110\n",
      "0000120 queries/test/embeddings/rnn_indxs_120\n",
      "--- 11392.461876153946 seconds ---\n",
      "\u001b[92m2023-06-15 01:55:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[2;32mread input file: candidates/test/input_dfm.yaml\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;31mGPU was requested but not available.\u001b[0m\n",
      "\u001b[92m2023-06-15 01:55:06\u001b[0m \u001b[95mSarahs-MacBook-Pro\u001b[0m \u001b[1m\u001b[90m[INFO]\u001b[0m \u001b[1;32mpytorch will use: cpu\u001b[0m\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from candidates/test/embeddings/rnn_fwd*\n",
      "0000000 candidates/test/embeddings/rnn_fwd_0\n",
      "0000010 candidates/test/embeddings/rnn_fwd_10\n",
      "0000020 candidates/test/embeddings/rnn_fwd_20\n",
      "0000030 candidates/test/embeddings/rnn_fwd_30\n",
      "0000040 candidates/test/embeddings/rnn_fwd_40\n",
      "0000050 candidates/test/embeddings/rnn_fwd_50\n",
      "0000060 candidates/test/embeddings/rnn_fwd_60\n",
      "0000070 candidates/test/embeddings/rnn_fwd_70\n",
      "0000080 candidates/test/embeddings/rnn_fwd_80\n",
      "0000090 candidates/test/embeddings/rnn_fwd_90\n",
      "0000100 candidates/test/embeddings/rnn_fwd_100\n",
      "0000110 candidates/test/embeddings/rnn_fwd_110\n",
      "0000120 candidates/test/embeddings/rnn_fwd_120\n",
      "0000130 candidates/test/embeddings/rnn_fwd_130\n",
      "0000140 candidates/test/embeddings/rnn_fwd_140\n",
      "0000150 candidates/test/embeddings/rnn_fwd_150\n",
      "0000160 candidates/test/embeddings/rnn_fwd_160\n",
      "0000170 candidates/test/embeddings/rnn_fwd_170\n",
      "0000180 candidates/test/embeddings/rnn_fwd_180\n",
      "0000190 candidates/test/embeddings/rnn_fwd_190\n",
      "0000200 candidates/test/embeddings/rnn_fwd_200\n",
      "0000210 candidates/test/embeddings/rnn_fwd_210\n",
      "0000220 candidates/test/embeddings/rnn_fwd_220\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 candidates/test/embeddings/rnn_indxs_0\n",
      "0000010 candidates/test/embeddings/rnn_indxs_10\n",
      "0000020 candidates/test/embeddings/rnn_indxs_20\n",
      "0000030 candidates/test/embeddings/rnn_indxs_30\n",
      "0000040 candidates/test/embeddings/rnn_indxs_40\n",
      "0000050 candidates/test/embeddings/rnn_indxs_50\n",
      "0000060 candidates/test/embeddings/rnn_indxs_60\n",
      "0000070 candidates/test/embeddings/rnn_indxs_70\n",
      "0000080 candidates/test/embeddings/rnn_indxs_80\n",
      "0000090 candidates/test/embeddings/rnn_indxs_90\n",
      "0000100 candidates/test/embeddings/rnn_indxs_100\n",
      "0000110 candidates/test/embeddings/rnn_indxs_110\n",
      "0000120 candidates/test/embeddings/rnn_indxs_120\n",
      "0000130 candidates/test/embeddings/rnn_indxs_130\n",
      "0000140 candidates/test/embeddings/rnn_indxs_140\n",
      "0000150 candidates/test/embeddings/rnn_indxs_150\n",
      "0000160 candidates/test/embeddings/rnn_indxs_160\n",
      "0000170 candidates/test/embeddings/rnn_indxs_170\n",
      "0000180 candidates/test/embeddings/rnn_indxs_180\n",
      "0000190 candidates/test/embeddings/rnn_indxs_190\n",
      "0000200 candidates/test/embeddings/rnn_indxs_200\n",
      "0000210 candidates/test/embeddings/rnn_indxs_210\n",
      "0000220 candidates/test/embeddings/rnn_indxs_220\n",
      "\n",
      "\n",
      "-- Combine vectors\n",
      "Reading vectors from candidates/test/embeddings/rnn_bwd*\n",
      "0000000 candidates/test/embeddings/rnn_bwd_0\n",
      "0000010 candidates/test/embeddings/rnn_bwd_10\n",
      "0000020 candidates/test/embeddings/rnn_bwd_20\n",
      "0000030 candidates/test/embeddings/rnn_bwd_30\n",
      "0000040 candidates/test/embeddings/rnn_bwd_40\n",
      "0000050 candidates/test/embeddings/rnn_bwd_50\n",
      "0000060 candidates/test/embeddings/rnn_bwd_60\n",
      "0000070 candidates/test/embeddings/rnn_bwd_70\n",
      "0000080 candidates/test/embeddings/rnn_bwd_80\n",
      "0000090 candidates/test/embeddings/rnn_bwd_90\n",
      "0000100 candidates/test/embeddings/rnn_bwd_100\n",
      "0000110 candidates/test/embeddings/rnn_bwd_110\n",
      "0000120 candidates/test/embeddings/rnn_bwd_120\n",
      "0000130 candidates/test/embeddings/rnn_bwd_130\n",
      "0000140 candidates/test/embeddings/rnn_bwd_140\n",
      "0000150 candidates/test/embeddings/rnn_bwd_150\n",
      "0000160 candidates/test/embeddings/rnn_bwd_160\n",
      "0000170 candidates/test/embeddings/rnn_bwd_170\n",
      "0000180 candidates/test/embeddings/rnn_bwd_180\n",
      "0000190 candidates/test/embeddings/rnn_bwd_190\n",
      "0000200 candidates/test/embeddings/rnn_bwd_200\n",
      "0000210 candidates/test/embeddings/rnn_bwd_210\n",
      "0000220 candidates/test/embeddings/rnn_bwd_220\n",
      "\n",
      "\n",
      "-- Combine IDs\n",
      "\n",
      "0000000 candidates/test/embeddings/rnn_indxs_0\n",
      "0000010 candidates/test/embeddings/rnn_indxs_10\n",
      "0000020 candidates/test/embeddings/rnn_indxs_20\n",
      "0000030 candidates/test/embeddings/rnn_indxs_30\n",
      "0000040 candidates/test/embeddings/rnn_indxs_40\n",
      "0000050 candidates/test/embeddings/rnn_indxs_50\n",
      "0000060 candidates/test/embeddings/rnn_indxs_60\n",
      "0000070 candidates/test/embeddings/rnn_indxs_70\n",
      "0000080 candidates/test/embeddings/rnn_indxs_80\n",
      "0000090 candidates/test/embeddings/rnn_indxs_90\n",
      "0000100 candidates/test/embeddings/rnn_indxs_100\n",
      "0000110 candidates/test/embeddings/rnn_indxs_110\n",
      "0000120 candidates/test/embeddings/rnn_indxs_120\n",
      "0000130 candidates/test/embeddings/rnn_indxs_130\n",
      "0000140 candidates/test/embeddings/rnn_indxs_140\n",
      "0000150 candidates/test/embeddings/rnn_indxs_150\n",
      "0000160 candidates/test/embeddings/rnn_indxs_160\n",
      "0000170 candidates/test/embeddings/rnn_indxs_170\n",
      "0000180 candidates/test/embeddings/rnn_indxs_180\n",
      "0000190 candidates/test/embeddings/rnn_indxs_190\n",
      "0000200 candidates/test/embeddings/rnn_indxs_200\n",
      "0000210 candidates/test/embeddings/rnn_indxs_210\n",
      "0000220 candidates/test/embeddings/rnn_indxs_220\n",
      "--- 11392.799102306366 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1236it [15:43,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total query: 1236 \n",
      "Correct correspondence count: 856\n",
      "Recall: 0.6925566343042071\n",
      "\n",
      "\n",
      "Total query: 1236\n",
      "Total retrieved: 1140\n",
      "Correct correspondence count: 856 \n",
      "Precision: 0.7508771929824561\n",
      "\n",
      "\n",
      "F-measure: 0.7205387205387206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    k_fold_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad6d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236.0 & 833.6666666666666 & 656.6666666666666 & 0.5312837108953613 & 0.7957508719917016 & 0.6275777197097023\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "with open('k_fold_validation/resultDict_faiss_3.pickle', 'rb') as handle:\n",
    "    resultDict = pickle.load(handle)\n",
    "    \n",
    "    total_instance = sum([resultDict[i]['instance'] for i in range(0,k)])\n",
    "    total_retrieved = sum([resultDict[i]['retrieved'] for i in range(0,k)])\n",
    "    total_correct = sum([resultDict[i]['correct'] for i in range(0,k)])\n",
    "    total_recall = sum([resultDict[i]['recall'] for i in range(0,k)])\n",
    "    total_precision = sum([resultDict[i]['precision'] for i in range(0,k)])\n",
    "    total_f_score = sum([resultDict[i]['f-score'] for i in range(0,k)])\n",
    "    \n",
    "    print(f\"{total_instance/k} & {total_retrieved/k} & {total_correct/k} & {total_recall/k} & {total_precision/k} & {total_f_score/k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "420b4550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'instance': 1236, 'retrieved': 698, 'correct': 580, 'recall': 0.4692556634304207, 'precision': 0.830945558739255, 'f-score': 0.5997931747673216}, 1: {'instance': 1236, 'retrieved': 663, 'correct': 534, 'recall': 0.4320388349514563, 'precision': 0.8054298642533937, 'f-score': 0.5624012638230648}, 2: {'instance': 1236, 'retrieved': 1140, 'correct': 856, 'recall': 0.6925566343042071, 'precision': 0.7508771929824561, 'f-score': 0.7205387205387206}, 3: {}, 4: {}}\n"
     ]
    }
   ],
   "source": [
    "with open('k_fold_validation/resultDict_faiss_3.pickle', 'rb') as handle:\n",
    "    resultDict = pickle.load(handle)\n",
    "    \n",
    "print(resultDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce42f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "{0: {'instance': 1236, 'retrieved': 698, 'correct': 580, 'recall': 0.4692556634304207, 'precision': 0.830945558739255, 'f-score': 0.5997931747673216}, 1: {'instance': 1236, 'retrieved': 663, 'correct': 534, 'recall': 0.4320388349514563, 'precision': 0.8054298642533937, 'f-score': 0.5624012638230648}, 2: {'instance': 1236, 'retrieved': 1140, 'correct': 856, 'recall': 0.6925566343042071, 'precision': 0.7508771929824561, 'f-score': 0.7205387205387206}, 3: {}, 4: {}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39deezy)",
   "language": "python",
   "name": "py39deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
